# Alertmanager configuration for Self-Healing Pipeline Guard
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@terragonlabs.com'
  smtp_auth_username: 'alerts@terragonlabs.com'
  smtp_auth_password: '${SMTP_PASSWORD}'
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# Templates for notification formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert distribution
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
    # Critical alerts go to multiple channels
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      group_interval: 5s
      repeat_interval: 5m
      
    # Security alerts get special handling
    - match:
        category: security
      receiver: 'security-alerts'
      group_wait: 0s
      repeat_interval: 15m
      
    # Warning alerts go to standard channels
    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 30m
      
    # Info alerts go to logs only
    - match:
        severity: info
      receiver: 'info-alerts'
      repeat_interval: 2h

# Inhibition rules to reduce noise
inhibit_rules:
  # If service is down, don't alert on high latency
  - source_match:
      alertname: ServiceDown
    target_match:
      alertname: HighAPILatency
    equal: ['service']
    
  # If critical error rate, don't alert on warning error rate
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal: ['alertname', 'service']

# Notification receivers
receivers:
  # Default webhook receiver
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://alertwebhook:5001/'
        send_resolved: true

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Severity:* {{ .Labels.severity }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true
        actions:
          - type: button
            text: 'View Dashboard'
            url: 'https://grafana.terragonlabs.com/d/healing-guard-overview'
          - type: button
            text: 'View Logs'
            url: 'https://kibana.terragonlabs.com'
    
    email_configs:
      - to: 'oncall@terragonlabs.com'
        subject: 'üö® CRITICAL ALERT: {{ .GroupLabels.alertname }}'
        body: |
          Critical alert triggered in Self-Healing Pipeline Guard:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Severity: {{ .Labels.severity }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
          {{ if .EndsAt }}Ended: {{ .EndsAt.Format "2006-01-02 15:04:05 UTC" }}{{ end }}
          
          Dashboard: https://grafana.terragonlabs.com/d/healing-guard-overview
          Logs: https://kibana.terragonlabs.com
          {{ end }}
    
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .GroupLabels.service }}'
        severity: 'critical'
        details:
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          source: 'Healing Guard Monitoring'
          timestamp: '{{ .CommonAnnotations.timestamp }}'

  # Security alerts
  - name: 'security-alerts'
    slack_configs:
      - channel: '#security-alerts'
        title: 'üîí SECURITY ALERT: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Security Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true
        color: 'danger'
    
    email_configs:
      - to: 'security@terragonlabs.com'
        subject: 'üîí SECURITY ALERT: {{ .GroupLabels.alertname }}'
        body: |
          Security alert detected in Self-Healing Pipeline Guard:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
          {{ end }}

  # Warning alerts
  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        title: '‚ö†Ô∏è WARNING: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Warning:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true
        color: 'warning'

  # Info alerts
  - name: 'info-alerts'
    slack_configs:
      - channel: '#alerts-info'
        title: '‚ÑπÔ∏è INFO: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Info:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          {{ end }}
        send_resolved: false
        color: 'good'

# Time-based silencing (maintenance windows)
# These can be managed via the Alertmanager UI or API